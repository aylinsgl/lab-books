{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83e\udde0 Lab Books","text":"<p>Welcome to my Lab Books, a collection of research projects I worked on during my PhD at the Scene Grammar Lab (2021\u20132025). This site serves as a living documentation of my work, including project descriptions, code, tutorials, and materials.</p> <p>\ud83d\udea7 Work in Progress: I'm continuously updating tutorials, refining explanations, and adding new content, so be sure to check back regularly!</p>"},{"location":"#what-youll-find-here","title":"\ud83d\udd0d What You\u2019ll Find Here","text":"<p>Each project has its own dedicated page, providing:</p> <ul> <li>\ud83d\udcd6 A detailed project description</li> <li>\ud83d\udda5\ufe0f Links to code repositories and datasets</li> <li>\ud83d\udcd1 Tutorials &amp; explanatory notebooks</li> <li>\ud83d\udd17 Additional resources and materials</li> </ul>"},{"location":"#project-overview","title":"Project Overview","text":"<p>Visual perception seems effortless \u2013 most of us do not feel like we expend much energy simply perceiving the world around us; we just do. However, inferring external world states from mere observations is a computationally complex task \u2013 like recognizing a friend\u2019s face in a crowded room or identifying a familiar object in poor lighting. This process involves transforming sensory information (light entering through the retina) into behaviorally relevant neural representations. Since perceiving our surroundings is vital, this transformation must happen quickly and automatically, despite being constrained by limited energy resources. During this process, the visual system must deal with ambiguity caused by, for example, limited views due to occlusion, poor lighting, or clutter, to form accurate, efficient, and robust representations of its surroundings. But the visual world is not random; there is regularity in the way light reflects off surfaces, as well as in how we design our environments, the objects we use, and how we arrange them.</p> <p>In the \"Scene Grammar\"  framework scenes are characterized by \u201cphrases\u201d: clusters of frequently co-occuring objects. At the top of the hierarchy lies the scene category. Anchor objects (e.g., toilet) at the top of the phrase hierarchy are predictive of the presence and location of surrounding smaller local objects (e.g., toilet paper) at the bottom of the hierarchy.</p> <p></p> <p>I examine the role of scene grammar through three approaches: analyzing features in images generated by Generative Adversarial Networks (GANs) to understand how the visual system utilizes anchor objects and the distribution of visual features in real-world scenes for rapid interpretation (Project 1). Second, by exploring the temporal dynamics of phrase-specific neural representations during object perception using EEG (Project 2). And third, by developing a computational model for scene grammar using graph autoencoders, which learn to embed scenes into low-dimensional representations based on the scene's objects and their relationships (Project 3). Together, these approaches provide insight into how scene grammar shapes both behavioral and neural responses during object and scene processing by influencing the formation of behaviorally relevant neural representations. This helps us understand how structured relationships between objects contribute to efficient and rapid visual perception.</p> <p></p>"},{"location":"#explore-the-projects","title":"\ud83d\ude80 Explore the Projects","text":"<p>Click on a project to learn more!</p> <ul> <li>Project 1</li> <li>Project 2</li> <li>Project 3</li> </ul>"},{"location":"projects/project-1/","title":"Project 1: Anchor objects drive realism while diagnostic objects drive categorization in GAN generated scenes","text":"<p>Read the paper: Link to paper</p>"},{"location":"projects/project-1/#description","title":"\ud83d\udcd6 Description","text":"<p>Our visual world is incredibly complex, yet we effortlessly make sense of it. How does the brain achieve this? This project explores how different objects within a scene contribute to our perception of realism and scene categorization. Specifically, we distinguish between anchor objects, which provide spatial cues for frequently co-occurring objects, and diagnostic objects, which help define the broader scene context.</p> <p>Using Generative Adversarial Networks (GANs) to generate diverse indoor scenes, we examined how human observers and deep neural networks (DNNs) process these elements. Our findings reveal that anchor objects and high-level visual features play a key role in determining a scene\u2019s realism, while diagnostic objects primarily drive categorization, even when realism is compromised. This highlights the brain\u2019s ability to flexibly rely on different sources of information to make sense of its surroundings.</p> <p>This project sheds light on how scene understanding emerges from the interplay between object-level information and broader spatial or semantic cues, offering insights into both human perception and artificial vision systems.</p>"},{"location":"projects/project-1/#experiment-overview-and-methodology","title":"Experiment Overview and Methodology","text":"<p>a) Real and Generated Scene Images. This study utilized both real and GAN-generated images to investigate scene perception. Real images were selected from the LSUN dataset, while generated images were produced using Progressive GANs, each trained on five different LSUN scene categories (Bedroom, Conference Room, Dining Room, Kitchen, Living Room). A total of 30 real and 30 generated images per category were used.</p> <p>b) Experimental Procedure. The study involved two experiments with different tasks but identical stimulus presentation:</p> <ul> <li>Experiment 1: Participants judged whether an image was real or generated, followed by a confidence rating (1-6).</li> <li>Experiment 2: Participants categorized the scene into one of five categories (5-alternative forced choice task), again followed  by a confidence rating.</li> </ul> <p>In both experiments, each trial followed a structured sequence, including a brief stimulus presentation (50 ms / 500 ms), dynamic masking (160 ms), and response collection.</p> <p>c) Scene Segmentation and Object Properties. To analyze object contributions to scene understanding, each image underwent automated scene segmentation using a neural network trained on scene segmentation. Identified objects were then matched with a database containing precomputed anchor frequency and diagnosticity scores. Anchor objects (e.g., a bed in a bedroom) provide spatial predictions for frequently co-occurring objects. Diagnostic objects (e.g., a bed in a bedroom, but not necessarily a chair) contribute to scene categorization. Each scene received a maximum anchor and diagnosticity score from its detected objects to quantify its structural and semantic composition.</p>"},{"location":"projects/project-1/#results-experiment-1-visual-features","title":"Results - Experiment 1 (Visual Features)","text":"<p>a) Predicting Realness Ratings from DNN Features - Feature maps were extracted from deep neural networks (DNNs) trained on scene and object classification. - Pretrained models (red) significantly outperformed randomly initialized models (black) in predicting realness ratings. - The x-axis represents model depth (earliest to deepest layers). - 95% confidence intervals were bootstrapped from 1000 resamples.</p> <p>b) Predicting Responses in the 2AFC Task - The same method was applied to predict responses in the signal detection task. - Results are shown separately for 50 ms and 500 ms conditions. - Statistical indicators include p-values and Bayes Factors for comparisons between trained and untrained networks.</p>"},{"location":"projects/project-1/#results-experiment-1-anchor-effect","title":"Results - Experiment 1 (Anchor Effect)","text":"<ul> <li>Left: Realness responses in the signal detection task (SDT) (1 = real, 0 = generated) show a significant effect of anchor status frequency.</li> <li>Right: Realness ratings also show a significant relationship with anchor status frequency.</li> </ul>"},{"location":"projects/project-1/#results-experiment-2-scene-categorization","title":"Results - Experiment 2 (Scene Categorization)","text":"<p>a) Predicting Categorization Performance from DNN Features</p> <ul> <li>We extracted layer-wise feature maps from deep neural networks (DNNs) trained on scene and object classification.</li> <li>Pretrained models (red) significantly outperformed randomly initialized models (black) in predicting categorization accuracy for generated images.</li> </ul> <p>b) Effects of Diagnosticity, Presentation Duration, and Realness</p> <ul> <li>Left: Categorization accuracy improves with higher diagnosticity, with significant differences at 50 ms.</li> <li>Right: Categorization accuracy is strongly correlated with realness (SDT Task), particularly at 50 ms.</li> </ul>"},{"location":"projects/project-1/#summary-of-findings","title":"Summary of Findings","text":"<p>Our study highlights the distinct roles of anchor and diagnostic objects in scene perception: Anchor objects play a crucial role in perceived realness, significantly influencing whether a scene is judged as real or generated. Diagnostic objects, while essential for scene categorization, had limited impact on realness judgments, as their function is more tied to category-specific information. Computational modeling revealed that high-level visual features, including object configurations, explained up to 60% of the variance in realness judgments. This emphasizes the importance of object parts and entire objects in early scene processing. We propose that anchor objects contribute to the distribution of high-level visual features in a scene due to their size and consistent spatial location. These findings offer new insights into how the visual system efficiently interprets scenes, leveraging the natural spatial and statistical structure of real-world environments.</p>"},{"location":"projects/project-1/#repository","title":"\ud83d\udda5\ufe0f Repository","text":"<p>All materials (experimental files, data, code) can be found via this OSF repository.</p>"},{"location":"projects/project-1/#tutorial-notebooks","title":"\ud83d\udcd1 Tutorial Notebooks","text":"<p>Coming soon...</p>"},{"location":"projects/project-2/","title":"Project 2: Object representations reflect hierarchical scene structure and depend on high-level visual, semantic, and action information.","text":"<p>Read the paper: Link to paper</p>"},{"location":"projects/project-2/#description","title":"\ud83d\udcd6 Description","text":"<p>This study examines how phrases\u2014clusters of local objects organized around an anchor\u2014are reflected in neural responses during object perception using EEG. Building on evidence that object and scene processing are highly interactive, the study explores whether objects within the same phrase elicit more similar neural activation patterns than those from different phrases within the same scene. Additionally, it investigates the shared features driving these representations and examines whether anchor and local objects exhibit similar neural feature encoding within scene grammar hierarchies.</p>"},{"location":"projects/project-2/#stimuli-and-experimental-design","title":"Stimuli and Experimental Design","text":"<p>a) Stimuli: The set included 8 object categories (10 exemplars each) from two scene categories (bathroom, kitchen), organized into four phrases, each containing an anchor and a local object. Colored boxes indicate object similarity levels.</p> <p>b) Task: Participants responded when a paperclip appeared. Trials containing paperclips were excluded from analysis.</p>"},{"location":"projects/project-2/#cross-decoding-methods-and-results","title":"Cross-Decoding Methods and Results","text":"<p>a) Multivariate Pattern Analysis (MVPA) Cross-Classification Framework: SVM classifiers with linear kernels were trained on either anchor or local objects and then tested on held-out trials. The classifier predictions were organized into confusion matrices for both training-to-testing directions.</p> <p>b) Cross-Classification Results: Generalization performance within phrases (between anchor and local objects) was compared to generalization between phrases within the same scene. The figure marks image onset and offset times, with shaded areas showing standard error. Horizontal bars indicate significant differences (within vs. between classification scores) based on one-sided cluster-based permutation t-tests (10,000 permutations, cluster-definition threshold at p &lt; 0.05).</p> <p>c) High-Level and Low-Level Predictor RDMs: High-level predictor RDMs include CORnet-IT, semantic similarity from GPT2 features, and implied actions. Low-level control RDMs consist of structural similarity (SSIM) and CORnet-S layer RDMs for V1, V2, and V4.</p>"},{"location":"projects/project-2/#rsa-results","title":"RSA results","text":"<p>We averaged correlation coefficients (semipartial correlation) from the significant phrase-specific cross-classification cluster and tested them against zero using one-sample t-tests (one-sided). Bonferroni correction was applied to p-values, and non-significant results were further examined with inferiority tests. In the boxplots, horizontal black bars indicate the median, points represent individual participants, and filled bars show noise ceilings.</p>"},{"location":"projects/project-2/#summary-of-findings","title":"Summary of Findings","text":"<p>Phrase-Specific Neural Representations: Objects within the same phrase share high-level semantic similarity and implied actions, beyond low-level visual features. These representations emerge early in visual processing (128\u2013164 ms), showing the brain\u2019s efficiency in integrating high-level features. </p> <p>Scene Grammar Hierarchy Effects: Anchor objects (top of hierarchy) activate feature sets shared with spatially, functionally, and semantically related objects, facilitating efficient perception. Local objects (bottom of hierarchy) engage feature sets linked to their anchor objects whose visual features are encoded in high-level representations of the local object. </p> <p>The contribution of GPT embeddings suggests that the way objects are structured in language reflects their real-world spatial and functional relationships, reinforcing the role of co-occurrence statistics in shaping object representations.</p>"},{"location":"projects/project-2/#repository","title":"\ud83d\udda5\ufe0f Repository","text":"<p>All code as well as preprocessed EEG files can be found via this github repository.</p>"},{"location":"projects/project-2/#tutorial-notebooks","title":"\ud83d\udcd1 Tutorial Notebooks","text":"<p>Coming soon...</p>"},{"location":"projects/project-3/","title":"Project 3: Predicting Human Perception of Scene Consistencies Using Graph Representations of Scene Grammar","text":"<p>Read the paper: Link to paper</p>"},{"location":"projects/project-3/#description","title":"\ud83d\udcd6 Description","text":"<p>This study uses graph autoencoders (GAEs) to learn low-dimensional embeddings that capture variations in object identity (\"what\") and spatial relationships (\"where\") in real-world scenes through unsupervised learning. Using the SCEGRAM database, which includes systematic violations of object identity and spatial relationships, the embeddings are evaluated on categorization and consistency rating tasks to assess whether they implicitly encode scene category and reflect human-like consistency effects. By comparing different semantic and spatial configurations, this study demonstrates a proof of concept for modeling scene grammar with interpretable graph-based representations.</p>"},{"location":"projects/project-3/#modelling-overview","title":"Modelling Overview","text":"<p>a) Unsupervised training with reconstruction and node feature loss. </p> <p>b) Experimental and control graph configurations. Four configurations were designed to capture varying degrees of scene information. </p> <p>c) Testing on SCEGRAM dataset. Embeddings from the SCEGRAM images are used as input features for downstream estimation tasks: scene classification (predicting scene categories) and predicting scene consistency ratings.</p>"},{"location":"projects/project-3/#classification-results","title":"Classification Results","text":"<p>a) Overall classification accuracy for each graph configuration (\u201cWhat\u201c, \u201cWhat &amp; Where\u201c, \u201cWhere\u201c, and \u201cControl\u201c). Bayes Factors are shown for comparisons against the control condition.</p> <p>b) Classification accuracy across individual consistency conditions (CON = consistent, SEM = semantic violation, SYN = syntactic violation, EXSYN = extreme syntactic violation, EXSEMSYN = extreme semantic and syntactic violation) for each graph configuration. Bayes factor values (BF) are shown as colored squares above each condition. The color intensity represents the strength of evidence, scaled logarithmically and visualized on a \u201cReds\u201c color scale. Bayes factors greater than 10, indicating strong evidence, are represented with progressively darker reds. For BF values less than or equal to 10, a light red color is used to denote weaker evidence. Error bars represent 95% confidence intervals, and the results are averaged over 50 training runs.</p>"},{"location":"projects/project-3/#ratings-results","title":"Ratings Results","text":"<p>a) Human consistency ratings across different scene conditions: CON (consistent), SEM (semantic violation), SYN (syntactic violation), EXSYN (extreme syntactic violation), and EXSEMSYN (extreme semantic and syntactic violation). Ratings reflect the degree to which scenes were perceived as consistent or inconsistent by human observers, with higher values indicating greater perceived inconsistency. </p> <p>b) Prediction accuracy of the graph autoencoder embeddings for scene consistency ratings across different graph configurations: \u201cWhat\u201d, \u201cWhat &amp; Where\u201d, \u201cWhere\u201d, and \u201cControl\u201d. Bayes factors (BF) indicate evidence strength for a positive correlation in each configuration. Error bars represent 95% confidence intervals. </p> <p>c-f) Contrast correlation results. (c,d) What configuration (e,f) \u201cWhat &amp; Where\u201d configuration. (c,e) Predicted consistency ratings for each scene condition across consistency conditions. (d,f) Correlation (Spearman) between the differences in human ratings and differences in embeddings for each condition contrast (e.g., CON vs. SEM). Positive correlations indicate that embedding differences align with rating differences for the given contrast. Bayes factors (BF) are displayed above each bar to indicate the strength of evidence for each correlation. Dots represent correlations for individual runs. Errorbars represent 95% confidence intervals.</p>"},{"location":"projects/project-3/#summary-of-findings","title":"Summary of Findings","text":"<ul> <li>What graph detected semantic inconsistencies.</li> <li>What &amp; Where graph captured structural consistency but not semantic violations.</li> <li>Where graph (spatial-only) failed to predict meaningful scene structure, emphasizing the importance of object identity in scene categorization and consistency expectations.</li> <li>Predicting consistency ratings was challenging due to their subjective nature, but embeddings partially approximated human judgments. </li> </ul> <p>Scene grammar as statistical appearance model: </p> <p></p> <p>a) Images from two scene categories (kitchen and living room), including one image with a semantic violation (toilet paper in a dishwasher). </p> <p>b) In pixel space, these images are not easily separable by category, and violations are not readily detectable. </p> <p>c) Graph representations capture the object-level structure of the scenes. </p> <p>d) Unsupervised learning on graph structures using a graph autoencoder learns scene grammar representations \u2013 statistical appearance models at the object level. </p> <p>e) These representations more explicitly separate image categories and are sensitive to violations of learned expectations about object identity and spatial relationships.</p>"},{"location":"projects/project-3/#repository","title":"\ud83d\udda5\ufe0f Repository","text":"<p>All code can be found via this github repository.</p>"},{"location":"projects/project-3/#tutorial-notebooks","title":"\ud83d\udcd1 Tutorial Notebooks","text":"<p>Coming soon...</p>"}]}