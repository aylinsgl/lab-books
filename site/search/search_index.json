{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83e\udde0 Lab Books","text":"<p>Welcome to my Lab Books, a collection of research projects I worked on during my PhD at the Scene Grammar Lab (2021\u20132025). This site serves as a living documentation of my work, including project descriptions, code, tutorials, and materials.</p> <p>\ud83d\udea7 Work in Progress: I'm continuously updating tutorials, refining explanations, and adding new content, so be sure to check back regularly!</p>"},{"location":"#what-youll-find-here","title":"\ud83d\udd0d What You\u2019ll Find Here","text":"<p>Each project has its own dedicated page, providing:</p> <ul> <li>\ud83d\udcd6 A detailed project description</li> <li>\ud83d\udda5\ufe0f Links to code repositories and datasets</li> <li>\ud83d\udcd1 Tutorials &amp; explanatory notebooks</li> <li>\ud83d\udd17 Additional resources and materials</li> </ul>"},{"location":"#project-overview","title":"Project Overview","text":"<ul> <li>Project 1: Examines how generative models implicitly learn natural scene \"ingredients\" by identifying key visual and scene grammar features that define a scene, based on human behavioral responses.</li> <li>Project 2: Investigates how scene grammar shapes neural object representations, focusing on temporal dynamics and shared representations within object phrases.</li> <li>Project 3: Develops graph-based computational models to encode scene grammar and validates them using human judgments of systematic scene grammar violations.</li> </ul>"},{"location":"#explore-the-projects","title":"\ud83d\ude80 Explore the Projects","text":"<p>Click on a project to learn more!</p> <ul> <li>Project 1</li> <li>Project 2</li> <li>Project 3</li> </ul>"},{"location":"projects/project-1/","title":"Project 1: Anchor objects drive realism while diagnostic objects drive categorization in GAN generated scenes","text":"<p>Read the paper: Link to paper</p>"},{"location":"projects/project-1/#description","title":"\ud83d\udcd6 Description","text":"<p>Our visual world is incredibly complex, yet we effortlessly make sense of it. How does the brain achieve this? This project explores how different objects within a scene contribute to our perception of realism and scene categorization. Specifically, we distinguish between anchor objects, which provide spatial cues for frequently co-occurring objects, and diagnostic objects, which help define the broader scene context.</p> <p>Using Generative Adversarial Networks (GANs) to generate diverse indoor scenes, we examined how human observers and deep neural networks (DNNs) process these elements. Our findings reveal that anchor objects and high-level visual features play a key role in determining a scene\u2019s realism, while diagnostic objects primarily drive categorization, even when realism is compromised. This highlights the brain\u2019s ability to flexibly rely on different sources of information to make sense of its surroundings.</p> <p>This project sheds light on how scene understanding emerges from the interplay between object-level information and broader spatial or semantic cues, offering insights into both human perception and artificial vision systems.</p>"},{"location":"projects/project-1/#experiment-overview-and-methodology","title":"Experiment Overview and Methodology","text":"<p>a) Real and Generated Scene Images. This study utilized both real and GAN-generated images to investigate scene perception. Real images were selected from the LSUN dataset, while generated images were produced using Progressive GANs, each trained on five different LSUN scene categories (Bedroom, Conference Room, Dining Room, Kitchen, Living Room). A total of 30 real and 30 generated images per category were used.</p> <p>b) Experimental Procedure. The study involved two experiments with different tasks but identical stimulus presentation:</p> <ul> <li>Experiment 1: Participants judged whether an image was real or generated, followed by a confidence rating (1-6).</li> <li>Experiment 2: Participants categorized the scene into one of five categories (5-alternative forced choice task), again followed  by a confidence rating.</li> </ul> <p>In both experiments, each trial followed a structured sequence, including a brief stimulus presentation (50 ms / 500 ms), dynamic masking (160 ms), and response collection.</p> <p>c) Scene Segmentation and Object Properties. To analyze object contributions to scene understanding, each image underwent automated scene segmentation using a neural network trained on scene segmentation. Identified objects were then matched with a database containing precomputed anchor frequency and diagnosticity scores. Anchor objects (e.g., a bed in a bedroom) provide spatial predictions for frequently co-occurring objects. Diagnostic objects (e.g., a bed in a bedroom, but not necessarily a chair) contribute to scene categorization. Each scene received a maximum anchor and diagnosticity score from its detected objects to quantify its structural and semantic composition.</p>"},{"location":"projects/project-1/#results-experiment-1-visual-features","title":"Results - Experiment 1 (Visual Features)","text":"<p>a) Predicting Realness Ratings from DNN Features - Feature maps were extracted from deep neural networks (DNNs) trained on scene and object classification. - Pretrained models (red) significantly outperformed randomly initialized models (black) in predicting realness ratings. - The x-axis represents model depth (earliest to deepest layers). - 95% confidence intervals were bootstrapped from 1000 resamples.</p> <p>b) Predicting Responses in the 2AFC Task - The same method was applied to predict responses in the signal detection task. - Results are shown separately for 50 ms and 500 ms conditions. - Statistical indicators include p-values and Bayes Factors for comparisons between trained and untrained networks.</p>"},{"location":"projects/project-1/#results-experiment-1-anchor-effect","title":"Results - Experiment 1 (Anchor Effect)","text":"<ul> <li>Left: Realness responses in the signal detection task (SDT) (1 = real, 0 = generated) show a significant effect of anchor status frequency.</li> <li>Right: Realness ratings also show a significant relationship with anchor status frequency.</li> </ul>"},{"location":"projects/project-1/#results-experiment-2-scene-categorization","title":"Results - Experiment 2 (Scene Categorization)","text":"<p>a) Predicting Categorization Performance from DNN Features</p> <ul> <li>We extracted layer-wise feature maps from deep neural networks (DNNs) trained on scene and object classification.</li> <li>Pretrained models (red) significantly outperformed randomly initialized models (black) in predicting categorization accuracy for generated images.</li> </ul> <p>b) Effects of Diagnosticity, Presentation Duration, and Realness</p> <ul> <li>Left: Categorization accuracy improves with higher diagnosticity, with significant differences at 50 ms.</li> <li>Right: Categorization accuracy is strongly correlated with realness (SDT Task), particularly at 50 ms.</li> </ul>"},{"location":"projects/project-1/#summary-of-findings","title":"Summary of Findings","text":"<p>Our study highlights the distinct roles of anchor and diagnostic objects in scene perception: Anchor objects play a crucial role in perceived realness, significantly influencing whether a scene is judged as real or generated. Diagnostic objects, while essential for scene categorization, had limited impact on realness judgments, as their function is more tied to category-specific information. Computational modeling revealed that high-level visual features, including object configurations, explained up to 60% of the variance in realness judgments. This emphasizes the importance of object parts and entire objects in early scene processing. We propose that anchor objects contribute to the distribution of high-level visual features in a scene due to their size and consistent spatial location. These findings offer new insights into how the visual system efficiently interprets scenes, leveraging the natural spatial and statistical structure of real-world environments.</p>"},{"location":"projects/project-1/#repository","title":"\ud83d\udda5\ufe0f Repository","text":"<p>All materials (experimental files, data, code) can be found via this OSF repository.</p>"},{"location":"projects/project-1/#tutorial-notebooks","title":"\ud83d\udcd1 Tutorial Notebooks","text":"<p>Coming soon...</p>"},{"location":"projects/project-2/","title":"Project 2: Object representations reflect hierarchical scene structure and depend on high-level visual, semantic, and action information.","text":"<p>Read the paper: Link to paper</p>"},{"location":"projects/project-2/#description","title":"\ud83d\udcd6 Description","text":"<p>This study examines how phrases\u2014clusters of local objects organized around an anchor\u2014are reflected in neural responses during object perception using EEG. Building on evidence that object and scene processing are highly interactive, the study explores whether objects within the same phrase elicit more similar neural activation patterns than those from different phrases within the same scene. Additionally, it investigates the shared features driving these representations and examines whether anchor and local objects exhibit similar neural feature encoding within scene grammar hierarchies.</p>"},{"location":"projects/project-2/#stimuli-and-experimental-design","title":"Stimuli and Experimental Design","text":"<p>a) Stimuli: The set included 8 object categories (10 exemplars each) from two scene categories (bathroom, kitchen), organized into four phrases, each containing an anchor and a local object. Colored boxes indicate object similarity levels.</p> <p>b) Task: Participants responded when a paperclip appeared. Trials containing paperclips were excluded from analysis.</p>"},{"location":"projects/project-2/#cross-decoding-methods-and-results","title":"Cross-Decoding Methods and Results","text":"<p>a) Multivariate Pattern Analysis (MVPA) Cross-Classification Framework: SVM classifiers with linear kernels were trained on either anchor or local objects and then tested on held-out trials. The classifier predictions were organized into confusion matrices for both training-to-testing directions.</p> <p>b) Cross-Classification Results: Generalization performance within phrases (between anchor and local objects) was compared to generalization between phrases within the same scene. The figure marks image onset and offset times, with shaded areas showing standard error. Horizontal bars indicate significant differences (within vs. between classification scores) based on one-sided cluster-based permutation t-tests (10,000 permutations, cluster-definition threshold at p &lt; 0.05).</p> <p>c) High-Level and Low-Level Predictor RDMs: High-level predictor RDMs include CORnet-IT, semantic similarity from GPT2 features, and implied actions. Low-level control RDMs consist of structural similarity (SSIM) and CORnet-S layer RDMs for V1, V2, and V4.</p>"},{"location":"projects/project-2/#rsa-results","title":"RSA results","text":"<p>We averaged correlation coefficients (semipartial correlation) from the significant phrase-specific cross-classification cluster and tested them against zero using one-sample t-tests (one-sided). Bonferroni correction was applied to p-values, and non-significant results were further examined with inferiority tests. In the boxplots, horizontal black bars indicate the median, points represent individual participants, and filled bars show noise ceilings.</p>"},{"location":"projects/project-2/#summary-of-findings","title":"Summary of Findings","text":"<p>Phrase-Specific Neural Representations: Objects within the same phrase share high-level semantic similarity and implied actions, beyond low-level visual features. These representations emerge early in visual processing (128\u2013164 ms), showing the brain\u2019s efficiency in integrating high-level features. </p> <p>Scene Grammar Hierarchy Effects: Anchor objects (top of hierarchy) activate feature sets shared with spatially, functionally, and semantically related objects, facilitating efficient perception. Local objects (bottom of hierarchy) engage feature sets linked to their anchor objects whose visual features are encoded in high-level representations of the local object. </p> <p>The contribution of GPT embeddings suggests that the way objects are structured in language reflects their real-world spatial and functional relationships, reinforcing the role of co-occurrence statistics in shaping object representations.</p>"},{"location":"projects/project-2/#repository","title":"\ud83d\udda5\ufe0f Repository","text":"<p>All code as well as preprocessed EEG files can be found via this github repository.</p>"},{"location":"projects/project-2/#tutorial-notebooks","title":"\ud83d\udcd1 Tutorial Notebooks","text":"<p>Coming soon...</p>"},{"location":"projects/project-3/","title":"Project 1: Graph-Based Scene Analysis","text":"<p>\ud83d\udccc Overview This project explores graph-based representations for scene understanding.</p> <p>\ud83d\udcc2 Repository: GitHub Link</p> <p>\ud83d\udcdd Notebook Tutorials: - Tutorial 1 - Tutorial 2</p>"}]}